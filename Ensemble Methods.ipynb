{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9609a6",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f79c43",
   "metadata": {},
   "source": [
    "Idea is to combine several weak learners into one model.\n",
    "\n",
    "A weak learner is usually a simple model that has:\n",
    "\n",
    "    - high variance\n",
    "    \n",
    "    - low accuracy\n",
    "    \n",
    "The most used weak learners are decision trees.\n",
    "\n",
    "There are a few ways to combine weak learners. Some of which include: max voting, averaging, weighted averaging etc.\n",
    "\n",
    "Big benefit of ensemble methods is variance reduction. By averaging over models which have high variance we hope the get sensible results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ae340",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "Train weaklearners on subsamples of the full dataset and combine their predictions. An example is *Random Forest*. Common practices:\n",
    "\n",
    "    - Bootstrapping subsample with replacement.\n",
    "    - Subsample features to reduce overfitting.\n",
    "    - Extra-Trees Ensemble is an alternative to random forest that uses the whole dataset but performs splits randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9c89a",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Train the current weaklearner where the previous one failed.\n",
    "\n",
    "    - Adaptive Boosting (AdaBoost): a sequence of decision trees where each tree weights higher the examples that were incorrectly classified by the previous tree.\n",
    "    \n",
    "    - Gradient Boosting: a sequence of models where the first model fits to the objective function and the next models fit to the residuals(gradients) of the previous model's output.\n",
    "    \n",
    "#### Gradient Boosting\n",
    "\n",
    "Formally we seek to minimize some loss function $L(y, F(x))$ where $F(x)$ are out predictions. Let $h_i(x)$ be the output of the i-th weak learner. Then, $F(x) = \\Sigma_{i=0}^m \\gamma_i h_i(x)$.\n",
    "\n",
    "1) Constant baseline: $F_0 = arg min_{\\gamma_0} L(y, \\gamma_0)$\n",
    "\n",
    "2) Each step is: $F_m(x) = F_{m-1}(x) + arg min_{h_m} L(y, F_{m-1}(x) + h_m(x))$\n",
    "\n",
    "3) Approximate the previous equation using gradients:\n",
    "$L(y, F_{m-1}(x) + h_m(x)) = L(y, F_{m-1}(x)) + \\nabla L(y, F_{m-1}) h_m(x)$\n",
    "\n",
    "4) Using gradient descent we have that $h_m(x)$ is proportional to  $-\\nabla L(y, F_{m-1})$\n",
    "\n",
    "5) Set $\\gamma_m = arg min_{\\gamma} L(y, F_{m-1}(x)) - \\gamma \\nabla L(y, F_{m-1}(x))$\n",
    "\n",
    "\n",
    "It is common to introduce some learning rate $0 < \\alpha \\leq 1$ and set $\\gamma_m \\rightarrow \\alpha \\cdot \\gamma_m$. This is called *shrinkage* and helps regularizing the model. Note that this parameter and the number of models play sort of inverse role. Roughly halving the learning rate has similar effect as doubling the number of models. This should be taken into account when searching for the optimal hyper parameters.\n",
    "\n",
    "Advanced gradient boosting methods are: XGBoost, LGBM and CatBoost. Instead of gradient descent they perform Newton-Rhapson which means that they use second-order Taylor expansion when fitting the next model. The implementations also feature parallization in performing decision tree splits, handling the missing data, automatically performing cross-validation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96866427",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "Similarly to the previous two methods, this method uses many not necessarally the same models and then combines their predictions. The combination is done by stacking a meta model on top of the previous models and fitting it with the inputs being predictons of the previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc6dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
